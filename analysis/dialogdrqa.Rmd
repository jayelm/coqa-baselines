---
title: "DialogDrQA"
output:
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
    toc_depth: 3
---

```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(message=FALSE, warning=FALSE, echo=FALSE)

source('./analysis.R')
```

In this project, we explore adapting standard reading comprehension models to the
multi-turn QA setting (CoQA, QuAC) with mechanisms for attending over the dialog
history, resolving coreferences, dialog state tracking, etc.

# 2018-11-07: Diagnosing attention

```{r 20181107_metrics}
metrics <- read_metrics(c(
  'q_dialog_attn_word_hidden_incr_linear_both_h250' = 'Current + History Gate (old)',
  'mask_answers_2' = 'Ignore Answers',
  'stateful_dialog' = 'State Carryover',
  'dialog_batched_2' = 'Baseline (dialog batching)',
  'danqi_baseline_exp' = 'Danqi Baseline'
))
ggplot(metrics, aes(x = epoch, y = value, group = model, color = model)) +
  geom_line() +
  xlab('Training Epoch') +
  ylab('Dev F1')
```

The main problem with the iffy performance and odd attention
maps for the past proposed methods was due to a bizarre focus on answer tokens.
Observe, for example, the emphasis on "Orange" in the following example:

```{r}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_both_h250")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")
```

In particular, there's a lot of (unnecessary) attention weight applied to the
*beginning* of the answer. I realized this is probably because answers are
encoded independently of questions, and the LSTM hidden/cell state is
initialized to 0 for each answer. This likely isn't what we want (we'd like to
carry over some context from the past question), and probably results in some weird
attention bias which gets exacerbated during training, resulting in the behavior above.

## Ignoring answers

As as simple check of this, I implemented a model (with the gate that looks at
both current AND past repr) which simply ignores past answers and attends over
question only. I get an F-score of **71.32**, which definitely outperforms the
dialog-batched baseline and is on par with Danqi's baseline. It's likely
additional gains are possible by attending over answers, and later by augmenting
document tokens with matches computed against these augmented question
representations.

When I look at the attention maps, the behavior isn't exactly what I was looking
for, but is actually sensible when you think about it. Here I've also plotted
the merge weights used to summarize the question into a fixed-length question
vector, as that really helps get to what's going on:

```{r mask_answers, echo=FALSE, warning=FALSE}
attn_0 <- read_attn("mask_answers_2")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")
```

Specifically there's actually very high keep probability assigned to most tokens
in the question, including the crucial pronouns (she), even though the
resolution seems to be performed correctly. But the keep probability assigned to
question marks (?) is generally much lower, and resolves to the past question's
question marks. This high keep probability makes sense given that the
merge weights barely even use intermediate tokens in the question - so there's
not much point of merging in the first place!

**This suggests that we should re-encode the augmented question representations
with another LSTM to better incorporate the attention.** Thish might make coreference
on intermediate tokens more useful (an experiment that is currently running).
One question is whether to do the linear merge first, then run an LSTM, or run
an LSTM on the concatted current/past attentions entirely?

At least in the current formulation, this suggests that maybe the model is primarily trying to do
sentence-level augmentation. This is what I had tried before, but with no
positive results, though I had never tried an incremental version (where the
question vectors are carried over), so that may be worth a try.

When the questions don't end with question marks, there is still a bias towards
resolving tokens at the end of a question to past tokens at the end of questions
(perhaps I should modify CoQA so every question ends with a question mark).

```{r}
attn_1 <- read_attn("mask_answers_2", 1)

attn_hm(attn_1, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_1, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_1, 5, hm_title = "Dev ex. 1, t = 6")
```

Finally, as Danqi suggested, here's an example of a single-word question: "What?" and the kinds of resolutions it tries to make:

```{r mask_answers_2, echo=FALSE, warning=FALSE}
attn_2 <- read_attn("mask_answers_2", 2)

attn_hm(attn_2, 1, hm_title = "Dev ex. 2, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 2, t = 3")
```

## Carrying RNN state over

Obviously ignoring answers is only a temporary fix, the ideal thing to do is not
to encode answers and questions separately, but rather have a dialog encoder
that runs over the entire QA sequence.

This requires some trickery: I can't just run the entire LSTM over
the dialog, since the backward pass lets future timesteps creep in. Instead, I
need to proceed QA-pair by QA-pair, running the LSTM, performing the attention,
and continuing to the next pair. I only carry forward the *forward* state of the
LSTM, and zero out the backward state (not sure if this is completely kosher).

Unfortunately I don't yet have final results for this run, as (1) it's much
slower, (2) my jobs keep getting killed. But so far, signs point to performance
being pretty similar to the model which ignores answers (as in, better than my
baseline). See the green line on the chart above.

The attention maps are a bit different: in particular, they're much more
diffuse. But also sensible in some places (we can discuss).

```{r state_carryover}
attn_0 <- read_attn("stateful_dialog")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")

attn_1 <- read_attn("stateful_dialog", 1)

attn_hm(attn_1, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_1, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_1, 5, hm_title = "Dev ex. 1, t = 6")

attn_2 <- read_attn("stateful_dialog", 2)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
```

# 2018-11-01: Incrementally attending to dialog history

Here are the scores of the models explored in this section. TL;DR no major improvements yet.

```{r 20181101_metrics}
metrics <- read_metrics(c(
  'q_dialog_attn_word_hidden_incr_linear_both_h250' = 'Current + History Gate',
  'q_dialog_attn_word_hidden_incr_linear_current_h250' = 'Current Gate',
  'q_dialog_attn_word_hidden_incr_avg_h250' = 'Average Gate',
  'dialog_batched_2' = 'Baseline (dialog batching)',
  'danqi_baseline_exp' = 'Danqi Baseline',
  'q_dialog_attn_word_hidden' = 'No Incremental'
))
ggplot(metrics, aes(x = epoch, y = value, group = model, color = model)) +
  geom_line() +
  xlab('Training Epoch') +
  ylab('Dev F1')
```

## Motivation

Here we describe the idea of "incrementally" attending to dialog history: at each question, update the question with dialog-sensitive representations, then update. We pitched this idea due to the **coreference chain** problem: for example, given the series of questions

1. What color was Cotton?
2. Where did she live?
3. Did she live alone?

We observed that when simply re-computing attention over hidden representations
for each question, the `she` in the 3rd question would (understandably) match
primarily to the `she` in the second question. However, the `she` in question 2
itself refers to `Cotton` in question 1! Thus there is a need for question
hidden representations to iteratively update themselves as time goes on.

## Problem formalization

At time $t$, we have a question $q^t$ consisting of $l_t$ tokens $\{q^t_1, \dots, q^t_l\}$ and a (gold-standard) answer
$a^t$ consisting of $n_t$ tokens $\{a^t_1, \dots, a^t_{n_t}\}$. We represent question tokens
$q^t$ as feature vectors $\{\tilde{\mathbf{q}}^t_1, \dots, \tilde{\mathbf{q}}^t_{l_t}\}$ and answer tokens likewise.
For now we just use their fastText word embeddings as features.

In standard DrQA, we run question token embeddings through a multi-layer BiLSTM
to obtain hidden representations for each token:

$$
\mathbf{q}^t = \{\mathbf{q}^t_1, \dots, \mathbf{q}^t_{l_t}\} = \text{BiLSTM}(\{\tilde{\mathbf{q}}^t_1, \dots, \tilde{\mathbf{q}}^t_{l_t}\})
$$

In the multi-turn setting, we will also run answer embeddings through the same
encoder. This is just to provide answer representations as dialog history for
future timesteps; DialogDrQA obviously does not see the answer to the current
question.

$$
\mathbf{a}^t = \{\mathbf{a}^t_1, \dots, \mathbf{a}^t_{n_t}\} = \text{BiLSTM}(\{\tilde{\mathbf{a}}^t_1, \dots, \tilde{\mathbf{a}}^t_{n_t}\})
$$

We are interested in augmenting these question hidden representations with
features aggregated across the dialog history up to time $t$, to generate
representations $\mathbf{q}^{t+}, \mathbf{a}^{t+}$. At $t = 1$, there is no dialog history, thus we simply set $\mathbf{q}^{t+} = \mathbf{q}^{t}$ and $\mathbf{a}^{t+} = \mathbf{a}^{t}$.
At $t > 1$, we have access to previous augmented representations $\mathbf{q}^{j+}, \mathbf{a}^{j+}$ for times $1 \leq j < t$.

To augment $\mathbf{q}^{t}, \mathbf{a}^{t}$, we first form the *dialog history* $\mathbf{d}^{t}$ up to time $t$ by concatenating past questions and answers:

\begin{align}
\mathbf{d}^{t} &= \{\mathbf{d}^{t}_1, \dots, \mathbf{d}^{t}_{h_t}\} \\ &= \{\mathbf{q}^{1}; \mathbf{a}^{1}; \dots; \mathbf{q}^{(t - 1)}; \mathbf{a}^{(t - 1)}\} \\
&= \{\mathbf{q}^{1}_1, \dots, \mathbf{q}^{1}_{l_1}, \mathbf{a}^{1}_1, \dots, \mathbf{a}^{1}_{n_1}, \dots, \dots, \mathbf{a}^{(t - 1)}_{n_{t - 1}}\}
\end{align}

While we compute attention with the unchanged hidden representations of the dialog history, crucially we perform augmentation with the already-augmented vectors $\mathbf{d}^{t+}$:

\begin{align}
\mathbf{d}^{t+} &= \{\mathbf{d}^{t+}_1, \dots, \mathbf{d}^{t+}_{h_t}\} \\ &= \{\mathbf{q}^{1+}; \mathbf{a}^{1+}; \dots; \mathbf{q}^{(t - 1)+}; \mathbf{a}^{(t - 1)+}\} \\
&= \{\mathbf{q}^{1+}_1, \dots, \mathbf{q}^{1+}_{l_1}, \mathbf{a}^{1+}_1, \dots, \mathbf{a}^{1+}_{n_1}, \dots, \dots, \mathbf{a}^{(t - 1)+}_{n_{t - 1}}\}
\end{align}

Then, for each token in the current question $\mathbf{q}^t_i$, we compute a weighted average $\mathbf{h}^t_i$ across tokens in the dialog history
$$
\mathbf{h}^t_i = \sum_j \alpha_{ij} \mathbf{d}^{t+}_{j}
$$
where $\alpha_{ij}$ are attention scores between $\mathbf{q}^t_i$ and every past *unaugmented* dialog token:
$$
\alpha_{ij} \propto \exp(\text{ReLU}(\mathbf{W} \mathbf{q}^t_i)^T \cdot \text{ReLU}(\mathbf{W} \mathbf{d}^t_j) + \theta(t - t_j))
$$
Here, $\mathbf{W}$ is a $k \times h$ matrix, where $k$ is the attention size and $h$ is the hidden representation size.

Since we want to favor more recent questions/answers in the dialog history, we
also include a linear *recency bias* parameter, $\theta$. Let $t_j$ be the
(absolute) timestep which dialog token $\mathbf{d}^t_j$ belongs to; then
$\theta$ downweights questions that occur earlier relative to $t$.

Thus, each pair $(\mathbf{q}^t_i, \mathbf{h}^t_i)$ respectively encodes the
current question token representation and (hopefully) the historical information
relevant for understanding the current token. What remains is to *merge*
the current and historical representations together:
$$
\mathbf{q}^{t+}_i = \text{merge}(\mathbf{q}^t_i, \mathbf{h}^t_i)
$$

**For now, we leave answer representations alone:** $\mathbf{a}^{t+}_i = \mathbf{a}^t_i$.

This augmented representation $\mathbf{q}^{t+}$ is what is used in the rest of
the traditional DrQA pipeline (in particular, forming single question vectors by
averaging across $\mathbf{q}^{t+}_i$ with self attention). In addition,
$\mathbf{q}^{t+}$ and $\mathbf{a}^{t+}$ are then used as part of the dialog
history for future timesteps.

## Defining the merge function

Here we present various formulations of the merge function, and what kind of attention maps result from training a model with that function.

### Average
We experiment with various ways of defining a merge function. First, one sensible approach is to simply do a standard average of the two vectors:

$$
\text{merge}(\mathbf{q}^t_i, \mathbf{h}^t_i) = (\mathbf{q}^t_i + \mathbf{h}^t_i) / 2
$$

which is akin to setting a constant *keep probability* $k_i = 0.5$ (see later examples).

#### Examples

In the following examples, the current question is labeled on the y axis; the
dialog history is labeled on the x axis. (each token prepended with time and
position in the q/a pair of the time). The far right column indicates the
(possibly learned) "keep" probability. Here, since there is a straight average,
the keep probability is always 0.5. Attention size $k$ is set to 250 for all experiments.

```{r avg_attn, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_avg_h250")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")
```

```{r avg_attn_2, echo=FALSE, warning=FALSE, cache=TRUE}
attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_avg_h250", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

### Gating

The next functions depend on learning a "gate" which decides to what degree to incorporate history. The gates output a *keep* value $k_i \in [0, 1]$, controlling the tradeoff between $\mathbf{q}^t_i$ and $\mathbf{h}^t_i$. A value of 1 means to keep the current representation, while a value of 0 means to use only the historical representation.

$$
\text{merge}(\mathbf{q}^t_i, \mathbf{h}^t_i) = k_i \mathbf{q}^t_i + (1 - k_i) \mathbf{h}^t_i
$$

#### Current word

This gate looks only at the representation of the current word when deciding to keep/forget:
$$k_i = \sigma(\mathbf{w}_k \cdot \mathbf{q}^t_i)$$
where $\mathbf{w}_k$ is a weight matrix of same dimensionality as $\mathbf{q}_t$ and $\sigma$ is the sigmoid function.

```{r linear_current_attn, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_h250")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")
```

```{r linear_current_attn_2, echo=FALSE, warning=FALSE, cache=TRUE}
attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_h250", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

#### Current word and past attention

This gate looks at \emph{both} the current and historical representations, concatenated together:
$$k_i = \sigma(\mathbf{w}_k \cdot [\mathbf{q}^t_i; \mathbf{h}^t_i])$$
where $\mathbf{w}_k$ has dimensionality $|\mathbf{q}^t_i| + |\mathbf{h}^t_i|$.

```{r linear_both_attn, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_both_h250")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")
```

```{r linear_both_attn_2, echo=FALSE, warning=FALSE, cache=TRUE}
attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_linear_both_h250", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

#### Other features for gates?

Other features may be useful, e.g. the attention weights $\alpha_{ij}$ (both
before and after softmax normalization). Additionally, right now these gates
learn keep probabilities independently for each question token. It may be
beneficial to use an RNN to output keep probabilities at each step, so there are
dependencies between the tokens.

## Other experiments

Here I run experiments with the gate that looks only at the current word, just
trying out different configurations:

### Limiting max history

Because there is a lot to attend to later in the dialog, here we limit the model
so that it can only look at that past 2 timesteps.

```{r linear_current_max_2, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_h250_max_2")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")

attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_h250_max_2", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

### Attending over answers

Previously I did not augment answer representations because the code was a bit
awkward; here I try that to see if that makes any difference.

```{r attend_answers, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_both_aa")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")

attn_2 <- read_attn("q_dialog_attn_both_aa", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

### Different attention scoring functions

#### No nonlinearity

Same as scoring function above, *without* ReLU nonlinearity (so just comparing raw dot product)

```{r bilinear, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_abilinear")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")

attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_abilinear", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

#### "Fully-aware" attention

(Huang et al., 2018)

```{r fully_aware, echo=FALSE, warning=FALSE, cache=TRUE}
attn_0 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_afully_aware")

attn_hm(attn_0, 1, hm_title = "Dev ex. 0, t = 2")
attn_hm(attn_0, 2, hm_title = "Dev ex. 0, t = 3")
attn_hm(attn_0, 5, hm_title = "Dev ex. 0, t = 6")

attn_2 <- read_attn("q_dialog_attn_word_hidden_incr_linear_current_afully_aware", 1)

attn_hm(attn_2, 1, hm_title = "Dev ex. 1, t = 2")
attn_hm(attn_2, 2, hm_title = "Dev ex. 1, t = 3")
attn_hm(attn_2, 5, hm_title = "Dev ex. 1, t = 6")
```

## Error analysis

Randomly sample 5 questions/attention heatmaps from the model gated by current and historical representations:

```{r error_analysis, cache=TRUE}
set.seed(0)
N_TO_ANALYZE <- 5
EXPT_TO_ANALYZE <- 'q_dialog_attn_both_aa'

coqa_dev <- read_coqa('../data/coqa/coqa-dev-v1.0.json')
preds <- read_predictions(EXPT_TO_ANALYZE)

wrong <- preds %>%
  filter(f1 == 0) %>%
  left_join(coqa_dev %>% select(id, story), by = c('id'))

wrong_n <- wrong %>%
  sample_n(N_TO_ANALYZE) %>%
  arrange(id, turn_id)

for (i in 1:N_TO_ANALYZE) {
  wrong_ex <- wrong_n[i, ]
  wrong_attn <- read_attn(EXPT_TO_ANALYZE, wrong_ex$id)
  cat("**Ex ", wrong_ex$id, ", turn ", wrong_ex$turn_id, "**")
  cat("\n\n")
  cat(wrong_ex$story[1])
  cat("\n\n")
  cat("**Predicted answer:** ", wrong_ex$answer[1], ", **gold answer:** ", wrong_ex$gold_answer_1[1])
  cat("\n\n")
  retrieve_q(wrong_attn, n = wrong_ex$turn_id)
  attn_hm(wrong_attn, n = wrong_ex$turn_id, max_history = 4)
}
```

Does the model get yes/no questions more wrong than other questions?

```{r}
yesno <- wrong %>%
  filter(answer %in% c('yes', 'no'))

preds %>%
  mutate(yesno = answer %in% c('yes', 'no')) %>%
  group_by(yesno) %>%
  summarise(mean_f1 = mean(f1)) %>%
  kable
```